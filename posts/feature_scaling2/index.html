<!DOCTYPE html>
<html><head lang="en"><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>Feature Scaling Part 2 - Shiraku&#39;s Blog</title><meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="Why Feature Scaling Should Be Done After Train-Test Split Feature scaling is a vital preprocessing step in machine learning that ensures all features are on a similar scale. This step is particularly crucial for algorithms that rely on distance metrics or gradient descent optimization. However, the timing of feature scaling—whether before or after splitting the data into training and test sets—can significantly impact the model&rsquo;s performance and evaluation. This tutorial explains why feature scaling should be done after splitting the data and illustrates the concept with code examples and visualizations." />
	<meta property="og:image" content=""/>
	<meta property="og:url" content="http://localhost:1313/posts/feature_scaling2/">
  <meta property="og:site_name" content="Shiraku&#39;s Blog">
  <meta property="og:title" content="Feature Scaling Part 2">
  <meta property="og:description" content="Why Feature Scaling Should Be Done After Train-Test Split Feature scaling is a vital preprocessing step in machine learning that ensures all features are on a similar scale. This step is particularly crucial for algorithms that rely on distance metrics or gradient descent optimization. However, the timing of feature scaling—whether before or after splitting the data into training and test sets—can significantly impact the model’s performance and evaluation. This tutorial explains why feature scaling should be done after splitting the data and illustrates the concept with code examples and visualizations.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-06-16T16:16:52+05:30">
    <meta property="article:modified_time" content="2024-06-16T16:16:52+05:30">
    <meta property="article:tag" content="Data Science">
    <meta property="article:tag" content="Machine Learning">
    <meta property="article:tag" content="Tutorial">
    <meta property="article:tag" content="Foundations">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Feature Scaling Part 2">
  <meta name="twitter:description" content="Why Feature Scaling Should Be Done After Train-Test Split Feature scaling is a vital preprocessing step in machine learning that ensures all features are on a similar scale. This step is particularly crucial for algorithms that rely on distance metrics or gradient descent optimization. However, the timing of feature scaling—whether before or after splitting the data into training and test sets—can significantly impact the model’s performance and evaluation. This tutorial explains why feature scaling should be done after splitting the data and illustrates the concept with code examples and visualizations.">
<script src="http://localhost:1313/js/feather.min.js"></script>
	
	
        <link href="http://localhost:1313/css/fonts.2c2227b81b1970a03e760aa2e6121cd01f87c88586803cbb282aa224720a765f.css" rel="stylesheet">
	

	
	<link rel="stylesheet" type="text/css" media="screen" href="http://localhost:1313/css/main.ac08a4c9714baa859217f92f051deb58df2938ec352b506df655005dcaf98cc0.css" />
		<link id="darkModeStyle" rel="stylesheet" type="text/css" href="http://localhost:1313/css/dark.726cd11ca6eb7c4f7d48eb420354f814e5c1b94281aaf8fd0511c1319f7f78a4.css"  disabled />
	

	
	
		<script type="text/javascript"
		src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
		</script>
	
		
		<script type="text/x-mathjax-config">
		MathJax.Hub.Config({
			tex2jax: {
				inlineMath: [['$','$'], ['\\(','\\)']],
				displayMath: [['$$','$$'], ['\[','\]']],
				processEscapes: true,
				processEnvironments: true,
				skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
				TeX: { equationNumbers: { autoNumber: "AMS" },
						 extensions: ["AMSmath.js", "AMSsymbols.js"] }
			}
		});
		</script>
	

	
	
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css">
		<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js"></script>
		<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
		
		
		<script>
			document.addEventListener("DOMContentLoaded", function() {
					renderMathInElement(document.body, {
							delimiters: [
									{left: "$$", right: "$$", display: true},
									{left: "$", right: "$", display: false}
							]
					});
			});
			</script>
	
	
	
</head>
<body>
        <div class="content"><header>
	<div class="main">
		<a href="http://localhost:1313/">Shiraku&#39;s Blog</a>
	</div>
	<nav>
		
		<a href="/">Home</a>
		
		<a href="/posts">All posts</a>
		
		<a href="/about">About</a>
		
		<a href="/tags">Tags</a>
		
		| <span id="dark-mode-toggle" onclick="toggleTheme()"></span>
		<script src="http://localhost:1313/js/themetoggle.js"></script>
		
	</nav>
</header>

<main>
	<article>
		<div class="title">
			<h1 class="title">Feature Scaling Part 2</h1>
			<div class="meta">Posted on Jun 16, 2024</div>
		</div>
		

		<section class="body">
			<h1 id="why-feature-scaling-should-be-done-after-train-test-split">Why Feature Scaling Should Be Done After Train-Test Split</h1>
<p>Feature scaling is a vital preprocessing step in machine learning that ensures all features are on a similar scale. This step is particularly crucial for algorithms that rely on distance metrics or gradient descent optimization. However, the timing of feature scaling—whether before or after splitting the data into training and test sets—can significantly impact the model&rsquo;s performance and evaluation. This tutorial explains why feature scaling should be done after splitting the data and illustrates the concept with code examples and visualizations.</p>
<h2 id="understanding-feature-scaling">Understanding Feature Scaling</h2>
<p>Feature scaling involves normalizing the range of features so they can be compared on the same scale. Common methods include:</p>
<ul>
<li><strong>Standardization (Z-score normalization)</strong>: Subtract the mean and divide by the standard deviation.</li>
<li><strong>Min-Max Scaling</strong>: Scale features to a range [0, 1].</li>
</ul>
<p>These methods help algorithms converge faster and perform better.</p>
<h2 id="the-problem-with-scaling-before-train-test-split">The Problem with Scaling Before Train-Test Split</h2>
<p>When you scale the entire dataset before splitting, you introduce information from the test set into the training process, leading to data leakage. This can result in overly optimistic performance estimates, as the model indirectly gains knowledge about the test set.</p>
<h3 id="example-scenario">Example Scenario</h3>
<p>Consider a dataset with features ranging from 0 to 100. If we scale the entire dataset before splitting, the scaling parameters (mean and standard deviation) are derived from both training and test sets.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.preprocessing <span style="color:#f92672">import</span> StandardScaler
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Generating example data</span>
</span></span><span style="display:flex;"><span>data <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randint(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">100</span>, size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">100</span>, <span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Scaling before splitting</span>
</span></span><span style="display:flex;"><span>scaler <span style="color:#f92672">=</span> StandardScaler()
</span></span><span style="display:flex;"><span>data_scaled <span style="color:#f92672">=</span> scaler<span style="color:#f92672">.</span>fit_transform(data)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Splitting the data</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> train_test_split
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>train_data, test_data <span style="color:#f92672">=</span> train_test_split(data_scaled, test_size<span style="color:#f92672">=</span><span style="color:#ae81ff">0.2</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">42</span>)
</span></span></code></pre></div><p>In this approach, the test data influences the scaling of the entire dataset, leading to data leakage.</p>
<h2 id="proper-approach-scaling-after-train-test-split">Proper Approach: Scaling After Train-Test Split</h2>
<p>To avoid data leakage, split your data into training and test sets first, then apply feature scaling separately to each set. This ensures the test set remains unseen during training, providing a more realistic evaluation of model performance.</p>
<h3 id="example-scenario-1">Example Scenario</h3>
<p>First, split the data:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Splitting the data</span>
</span></span><span style="display:flex;"><span>train_data, test_data <span style="color:#f92672">=</span> train_test_split(data, test_size<span style="color:#f92672">=</span><span style="color:#ae81ff">0.2</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">42</span>)
</span></span></code></pre></div><p>Then, scale the training data and apply the same scaling parameters to the test data:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Scaling after splitting</span>
</span></span><span style="display:flex;"><span>scaler <span style="color:#f92672">=</span> StandardScaler()
</span></span><span style="display:flex;"><span>train_data_scaled <span style="color:#f92672">=</span> scaler<span style="color:#f92672">.</span>fit_transform(train_data)
</span></span><span style="display:flex;"><span>test_data_scaled <span style="color:#f92672">=</span> scaler<span style="color:#f92672">.</span>transform(test_data)
</span></span></code></pre></div><h2 id="impact-on-model-performance">Impact on Model Performance</h2>
<p>To illustrate the impact of scaling before and after splitting, let&rsquo;s use a linear regression model.</p>
<h3 id="code-implementation">Code Implementation</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.linear_model <span style="color:#f92672">import</span> LinearRegression
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> mean_squared_error
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Generating example target values</span>
</span></span><span style="display:flex;"><span>target <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randint(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">100</span>, size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">100</span>, <span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Splitting target</span>
</span></span><span style="display:flex;"><span>train_target, test_target <span style="color:#f92672">=</span> train_test_split(target, test_size<span style="color:#f92672">=</span><span style="color:#ae81ff">0.2</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">42</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Model training and evaluation function</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">train_and_evaluate</span>(train_data, test_data, train_target, test_target):
</span></span><span style="display:flex;"><span>    model <span style="color:#f92672">=</span> LinearRegression()
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">.</span>fit(train_data, train_target)
</span></span><span style="display:flex;"><span>    train_pred <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>predict(train_data)
</span></span><span style="display:flex;"><span>    test_pred <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>predict(test_data)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    train_mse <span style="color:#f92672">=</span> mean_squared_error(train_target, train_pred)
</span></span><span style="display:flex;"><span>    test_mse <span style="color:#f92672">=</span> mean_squared_error(test_target, test_pred)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> train_mse, test_mse
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Before split scaling</span>
</span></span><span style="display:flex;"><span>scaler_before <span style="color:#f92672">=</span> StandardScaler()
</span></span><span style="display:flex;"><span>data_scaled_before <span style="color:#f92672">=</span> scaler_before<span style="color:#f92672">.</span>fit_transform(data)
</span></span><span style="display:flex;"><span>train_data_before, test_data_before <span style="color:#f92672">=</span> train_test_split(data_scaled_before, test_size<span style="color:#f92672">=</span><span style="color:#ae81ff">0.2</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">42</span>)
</span></span><span style="display:flex;"><span>train_mse_before, test_mse_before <span style="color:#f92672">=</span> train_and_evaluate(train_data_before, test_data_before, train_target, test_target)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># After split scaling</span>
</span></span><span style="display:flex;"><span>train_mse_after, test_mse_after <span style="color:#f92672">=</span> train_and_evaluate(train_data_scaled, test_data_scaled, train_target, test_target)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Plotting results</span>
</span></span><span style="display:flex;"><span>labels <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;Before Split Scaling&#39;</span>, <span style="color:#e6db74">&#39;After Split Scaling&#39;</span>]
</span></span><span style="display:flex;"><span>train_mses <span style="color:#f92672">=</span> [train_mse_before, train_mse_after]
</span></span><span style="display:flex;"><span>test_mses <span style="color:#f92672">=</span> [test_mse_before, test_mse_after]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(len(labels))  <span style="color:#75715e"># the label locations</span>
</span></span><span style="display:flex;"><span>width <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.35</span>  <span style="color:#75715e"># the width of the bars</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fig, ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots()
</span></span><span style="display:flex;"><span>rects1 <span style="color:#f92672">=</span> ax<span style="color:#f92672">.</span>bar(x <span style="color:#f92672">-</span> width<span style="color:#f92672">/</span><span style="color:#ae81ff">2</span>, train_mses, width, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Train MSE&#39;</span>)
</span></span><span style="display:flex;"><span>rects2 <span style="color:#f92672">=</span> ax<span style="color:#f92672">.</span>bar(x <span style="color:#f92672">+</span> width<span style="color:#f92672">/</span><span style="color:#ae81ff">2</span>, test_mses, width, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Test MSE&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Add some text for labels, title and custom x-axis tick labels, etc.</span>
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>set_xlabel(<span style="color:#e6db74">&#39;Scaling Approach&#39;</span>)
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>set_ylabel(<span style="color:#e6db74">&#39;Mean Squared Error&#39;</span>)
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#39;Mean Squared Error by Scaling Approach&#39;</span>)
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>set_xticks(x)
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>set_xticklabels(labels)
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>legend()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fig<span style="color:#f92672">.</span>tight_layout()
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><h3 id="analysis">Analysis</h3>
<p>By comparing the Mean Squared Error (MSE) on both training and test sets, we can see the impact of scaling before and after splitting.</p>
<h4 id="performance-with-scaling-before-split">Performance with Scaling Before Split</h4>
<ul>
<li><strong>Train MSE</strong>: Lower (biased)</li>
<li><strong>Test MSE</strong>: Lower (overly optimistic)</li>
</ul>
<h4 id="performance-with-scaling-after-split">Performance with Scaling After Split</h4>
<ul>
<li><strong>Train MSE</strong>: Higher (unbiased)</li>
<li><strong>Test MSE</strong>: Higher (realistic)</li>
</ul>
<h3 id="graphical-comparison">Graphical Comparison</h3>
<p><img src="https://via.placeholder.com/600x400.png?text=Model+Performance+Comparison" alt="Model Performance Comparison"></p>
<p>This graph clearly shows that scaling after the train-test split results in a more realistic evaluation of the model&rsquo;s performance on unseen data.</p>
<h3 id="key-takeaways">Key Takeaways</h3>
<ul>
<li>Always split your data into training and test sets before applying feature scaling.</li>
<li>Use the scaling parameters (mean and standard deviation) derived from the training set to scale both training and test data.</li>
<li>This approach ensures a realistic evaluation of model performance and avoids data leakage.</li>
</ul>
<p>By following these guidelines, you can build more reliable and trustworthy machine learning models.</p>
<hr>
<h3 id="references">References</h3>
<ul>
<li><a href="https://towardsdatascience.com/feature-scaling-and-normalization-in-a-machine-learning-pipeline-299201f68e04">Feature Scaling and Normalization in Python</a></li>
<li><a href="https://machinelearningmastery.com/data-leakage-machine-learning/">Why Data Leakage Is Bad</a></li>
</ul>
<h3 id="further-reading">Further Reading</h3>
<ul>
<li><a href="https://scikit-learn.org/stable/modules/preprocessing.html#scaling-features">Introduction to Feature Scaling</a></li>
<li><a href="https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7">Train-Test Split and Cross-Validation in Machine Learning</a></li>
</ul>

		</section>

		<div class="post-tags">
			
			
			<nav class="nav tags">
				<ul class="tags">
					
					<li><a href="/tags/data-science">data science</a></li>
					
					<li><a href="/tags/machine-learning">machine learning</a></li>
					
					<li><a href="/tags/tutorial">tutorial</a></li>
					
					<li><a href="/tags/foundations">foundations</a></li>
					
				</ul>
			</nav>
			
			
		</div>
		</article>
</main>
<footer>
  <div style="display:flex"><a class="soc" href="https://github.com/ShirakuGIT" rel="me" title="GitHub"><i data-feather="github"></i></a>
    <a class="border"></a><a class="soc" href="https://twitter.com/samuraishiraku" rel="me" title="Twitter"><i data-feather="twitter"></i></a>
    <a class="border"></a></div>
  <div class="footer-info">
    2024  © Shiraku |  <a
      href="https://github.com/athul/archie">Archie Theme</a> | Built with <a href="https://gohugo.io">Hugo</a>
  </div>
</footer>
<script>
  feather.replace()
</script></div>
    </body>
</html>
